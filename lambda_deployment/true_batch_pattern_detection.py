#!/usr/bin/env python3
"""
TRUE Batch Pattern Detection - Send ALL claims to Ollama in ONE request.

Similar to true_batch_reasoning.py but for pattern detection.
"""

import json
import time
from typing import List, Dict, Any
from pydantic import BaseModel, Field
from strands import Agent
#from strands.models.ollama import OllamaModel


class PatternMatch(BaseModel):
    """A detected misinformation pattern."""
    pattern: str = Field(description="Pattern name (e.g., 'emotional_manipulation', 'false_authority')")
    confidence: float = Field(description="Confidence score 0.0-1.0")
    example: str = Field(description="Specific text snippet that triggered this pattern")


class SingleClaimPatterns(BaseModel):
    """Pattern detection result for a single claim."""
    claim_id: int = Field(description="The numeric ID of the claim (1-based)")
    patterns_detected: List[PatternMatch] = Field(description="List of detected patterns")
    severity_score: float = Field(description="Overall severity score 0.0-1.0")
    manipulation_techniques: List[str] = Field(description="List of manipulation technique names")


class BatchPatternResult(BaseModel):
    """Results for a batch of claims."""
    results: List[SingleClaimPatterns]


async def true_batch_pattern_detection(
    claims: List[str],
    ollama_endpoint: str = None,
    ollama_model: str = None,
    agent_config = None
) -> List[Dict[str, Any]]:
    """
    Send ALL claims to LLM in ONE request for TRUE batch pattern detection.
    
    Supports both Ollama and Bedrock providers.
    
    Args:
        claims: List of claim texts to analyze
        ollama_endpoint: Ollama server endpoint (for Ollama provider)
        ollama_model: Model to use (for Ollama provider)
        agent_config: AgentConfig object (alternative to individual params)
        
    Returns:
        List of pattern detection results, one per claim
    """
    start_time = time.time()
    
    if not claims:
        return []
    
    # Determine provider and initialize appropriate model
    if agent_config:
        from agents.common.agent_config import get_llm_agent
        agent = get_llm_agent(agent_config)
        provider = agent_config.llm_provider
        print(f"üîç TRUE BATCH PATTERNS: Sending {len(claims)} claims to {provider.upper()} in ONE request...")
    else:
        # Fallback to Ollama with provided params
        print(f"üîç TRUE BATCH PATTERNS: Sending {len(claims)} claims to Ollama in ONE request...")
        #from strands.models.ollama import OllamaModel
        raise ValueError("Ollama provider requires agent_config parameter")
        # ollama = OllamaModel(
        #     host=ollama_endpoint,
        #     model_id=ollama_model
        # )
        agent = Agent(
            model=ollama,
            system_prompt="You are an AI that detects misinformation patterns. Respond ONLY with the JSON structure requested."
        )
    
    # Build ONE prompt with ALL claims
    prompt_claims = "\n".join([f"Claim {i+1}: \"{claim}\"" for i, claim in enumerate(claims)])
    
    prompt = (
        f"For each of the following {len(claims)} claims, detect misinformation patterns:\n\n"
        "Common patterns to look for:\n"
        "- emotional_manipulation: Appeals to fear, anger, or outrage\n"
        "- false_authority: Fake experts or misrepresented credentials\n"
        "- cherry_picking: Selective use of data\n"
        "- conspiracy_theory: Unfalsifiable conspiracy narratives\n"
        "- false_dichotomy: Presenting only two extreme options\n"
        "- ad_hominem: Attacking people instead of arguments\n"
        "- strawman: Misrepresenting opposing views\n"
        "- bandwagon: Everyone believes it so it must be true\n"
        "- slippery_slope: One thing will inevitably lead to extreme outcomes\n"
        "- anecdotal_evidence: Using personal stories as proof\n\n"
        "For each claim:\n"
        "1. List detected patterns with confidence scores\n"
        "2. Provide specific text examples\n"
        "3. Calculate severity score (0.0-1.0)\n"
        "4. List manipulation techniques used\n\n"
        "Respond with ONLY a JSON object in this EXACT format (use straight quotes \" not curly quotes):\n"
        "{\n"
        '  "results": [\n'
        '    {"claim_id": 1, "patterns_detected": [{"pattern": "emotional_manipulation", "confidence": 0.8, "example": "text snippet"}], "severity_score": 0.7, "manipulation_techniques": ["fear_mongering"]},\n'
        '    {"claim_id": 2, "patterns_detected": [], "severity_score": 0.0, "manipulation_techniques": []}\n'
        "  ]\n"
        "}\n"
        "IMPORTANT: Do NOT use curly quotes (" ") or apostrophes (' ') in your response. Use only straight quotes (\").\n\n"
        f"{prompt_claims}"
    )
    
    try:
        # ONE call with structured_output (like kg_building_agent.py)
        llm_start = time.time()
        
        try:
            agent_response = agent.structured_output(BatchPatternResult, prompt)
            llm_time = (time.time() - llm_start) * 1000
            batch_result = agent_response
        except Exception as e:
            # Catch the strands bug where results field is a string
            if "Input should be a valid list" in str(e) and "input_value=" in str(e):
                # Extract the response dict from the error and manually parse the results field
                llm_time = (time.time() - llm_start) * 1000
                # The error happens because Bedrock returns results as a JSON string
                # We need to get the raw response and parse it ourselves
                # For now, return empty results
                print(f"‚ö†Ô∏è Bedrock returned results as string (strands library bug), returning empty")
                batch_result = BatchPatternResult(results=[])
            else:
                raise e
        
        # Check if the agent returned a string instead of a parsed object (like kg_building_agent.py)
        if isinstance(batch_result, str):
            try:
                # Parse the JSON string into a Python dictionary
                parsed_data = json.loads(batch_result)
                # Create the Pydantic model from the parsed dictionary
                batch_result = BatchPatternResult(**parsed_data)
            except (json.JSONDecodeError, TypeError) as e:
                print(f"‚ö†Ô∏è Failed to parse agent's string response: {e}")
                batch_result = BatchPatternResult(results=[])
        
        # Convert to standard format
        results = []
        for res in batch_result.results:
            claim_index = res.claim_id - 1  # Convert 1-based to 0-based
            if 0 <= claim_index < len(claims):
                # Convert patterns to dict format
                patterns = [
                    {
                        "pattern": p.pattern,
                        "confidence": p.confidence,
                        "examples": [p.example] if p.example else []
                    }
                    for p in res.patterns_detected
                ]
                
                results.append({
                    "patterns_detected": patterns,
                    "severity_score": res.severity_score,
                    "manipulation_techniques": res.manipulation_techniques,
                    "execution_time_ms": llm_time / len(claims)  # Distribute time
                })
        
        total_time = (time.time() - start_time) * 1000
        avg_per_claim = total_time / len(claims)
        
        print(f"‚úÖ TRUE BATCH PATTERNS complete: {len(results)} claims in {total_time:.0f}ms ({avg_per_claim:.0f}ms per claim)")
        print(f"   Total patterns detected: {sum(len(r['patterns_detected']) for r in results)}")
        
        return results
        
    except Exception as e:
        print(f"‚ùå TRUE BATCH PATTERNS failed: {e}")
        import traceback
        traceback.print_exc()
        
        # Return empty results
        return [
            {
                "patterns_detected": [],
                "severity_score": 0.0,
                "manipulation_techniques": [],
                "error": str(e),
                "execution_time_ms": 0
            }
            for _ in claims
        ]


# Test function
async def test_true_batch_patterns():
    """Test TRUE batch pattern detection."""
    test_claims = [
        "BREAKING: Scientists SHOCKED by this discovery! Big Pharma doesn't want you to know!",
        "Water boils at 100¬∞C at sea level",
        "They're hiding the truth from us! Wake up sheeple!",
        "According to peer-reviewed research published in Nature...",
        "My friend's cousin said vaccines made their child sick, so they must be dangerous!"
    ]
    
    print("\n" + "="*80)
    print("Testing TRUE Batch Pattern Detection")
    print("="*80)
    
    results = await true_batch_pattern_detection(test_claims)
    
    print("\nResults:")
    for i, (claim, result) in enumerate(zip(test_claims, results), 1):
        print(f"\n{i}. {claim[:80]}...")
        print(f"   Patterns: {len(result['patterns_detected'])}")
        for pattern in result['patterns_detected']:
            print(f"      - {pattern['pattern']} (confidence: {pattern['confidence']:.2f})")
        print(f"   Severity: {result['severity_score']:.2f}")
        print(f"   Techniques: {', '.join(result['manipulation_techniques'][:3])}")


if __name__ == "__main__":
    import asyncio
    asyncio.run(test_true_batch_patterns())
